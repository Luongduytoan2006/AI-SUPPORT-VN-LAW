import os, time, requests
from pathlib import Path
from flask import Flask, send_from_directory, request, jsonify
import google.generativeai as genai

from core.pipeline import load_index, answer_question
from core.settings import Settings
from core.utils import print_status_info, print_step_timing, print_timing_info, check_internet_connection

ROOT = Path(__file__).resolve().parents[1]  # project root
APP_DIR = ROOT / "app"

app = Flask(__name__, static_folder=str(APP_DIR), template_folder=str(APP_DIR))
app.secret_key = os.getenv('SECRET_KEY', 'vn-legal-assistant-2024')

settings = Settings()

# -----------------------------
# Helpers
# -----------------------------
def _online() -> bool:
    return check_internet_connection()

def _gemini_enabled() -> bool:
    return bool(os.getenv("GOOGLE_API_KEY", "").strip())

def _citations_to_context(citations) -> str:
    buf = []
    for c in citations or []:
        title  = c.get("title","")
        art    = c.get("article","")
        clause = c.get("clause")
        text   = (c.get("text") or "").strip()
        src    = c.get("source","")
        tag = f"[{title} | Äiá»u {art}" + (f", Khoáº£n {clause}]" if clause else "]")
        buf.append(f"{tag}\n{text}\nSOURCE: {src}")
    return "\n---\n".join(buf)

def _head(s: str, n: int = 50) -> str:
    s = (s or "").strip().replace("\n", " ")
    return s[:n]

def _gemini_answer(question: str, context: str) -> str | None:
    key = os.getenv("GOOGLE_API_KEY", "").strip()
    if not key:
        return None
    model_id = os.getenv("GEMINI_MODEL", "gemini-2.0-flash")
    genai.configure(api_key=key)
    prompt = f'''
Báº¡n lÃ  Luáº­t sÆ° tÆ° váº¥n phÃ¡p luáº­t Viá»‡t Nam chuyÃªn nghiá»‡p. TÆ° váº¥n dá»±a trÃªn CONTEXT chÃ­nh xÃ¡c vÃ  thá»±c tiá»…n.

NGUYÃŠN Táº®C:
- CHá»ˆ sá»­ dá»¥ng thÃ´ng tin cÃ³ trong CONTEXT
- Má»—i káº¿t luáº­n pháº£i cÃ³ trÃ­ch dáº«n Luáº­t Z | Äiá»u X, khoáº£n Y
VD: Theo quy Ä‘á»‹nh [hon_nhan | Äiá»u 3a, Khoáº£n 1], nam tá»« Ä‘á»§ 20 tuá»•i trá»Ÿ lÃªn vÃ  ná»¯ tá»« Ä‘á»§ 18 tuá»•i trá»Ÿ lÃªn thÃ¬ Ä‘Æ°á»£c phÃ©p káº¿t hÃ´n. NhÆ° váº­y, báº¡n 21 tuá»•i thÃ¬ Ä‘Ã¡p á»©ng yÃªu cáº§u vá» Ä‘á»™ tuá»•i Ä‘á»ƒ káº¿t hÃ´n. => Sai
ÄÃºng => Theo quy Ä‘á»‹nh luáº­t hÃ´n nhÃ¢n Ä‘iá»u 3a, khoáº£n 1, nam tá»« Ä‘á»§ 20 tuá»•i trá»Ÿ lÃªn vÃ  ná»¯ tá»« Ä‘á»§ 18 tuá»•i trá»Ÿ lÃªn thÃ¬ Ä‘Æ°á»£c phÃ©p káº¿t hÃ´n. NhÆ° váº­y, báº¡n 21 tuá»•i thÃ¬ Ä‘Ã¡p á»©ng yÃªu cáº§u vá» Ä‘á»™ tuá»•i Ä‘á»ƒ káº¿t hÃ´n.
- Tráº£ lá»i Ä‘áº§y Ä‘á»§, chi tiáº¿t nhÆ° luáº­t sÆ° chuyÃªn nghiá»‡p. 
- Náº¿u thiáº¿u thÃ´ng tin: ghi "Cáº§n tham kháº£o thÃªm" + nÃªu rÃµ yÃªu cáº§u há»i láº¡i cÃ¢u há»i Ä‘áº§y Ä‘á»§, bá»• sung nhá»¯ng Ä‘iá»u cÃ²n thiáº¿u, cáº§n gÃ¬

Cáº¤U TRÃšC MARKDOWN:

# Káº¿t luáº­n nhanh
- 2-4 Ä‘iá»ƒm chÃ­nh vá»›i trÃ­ch dáº«n

# PhÃ¢n tÃ­ch phÃ¡p lÃ½ chi tiáº¿t
## Quy Ä‘á»‹nh phÃ¡p luáº­t
- NÃªu rÃµ cÃ¡c Ä‘iá»u luáº­t Ã¡p dá»¥ng vá»›i trÃ­ch dáº«n Ä‘áº§y Ä‘á»§
- Giáº£i thÃ­ch Ã½ nghÄ©a vÃ  cÃ¡ch hiá»ƒu

## Ãp dá»¥ng thá»±c tiá»…n  
- HÆ°á»›ng dáº«n cá»¥ thá»ƒ cÃ¡ch thá»±c hiá»‡n
- CÃ¡c trÆ°á»ng há»£p Ä‘áº·c biá»‡t (náº¿u cÃ³)
- LÆ°u Ã½ quan trá»ng

# HÆ°á»›ng dáº«n thá»±c hiá»‡n (náº¿u Ã¡p dá»¥ng)
- CÃ¡c bÆ°á»›c cáº§n lÃ m
- Thá»§ tá»¥c, giáº¥y tá» cáº§n thiáº¿t
- CÆ¡ quan cÃ³ tháº©m quyá»n

# CÄƒn cá»© phÃ¡p lÃ½ Ä‘Ã£ Ã¡p dá»¥ng
- Liá»‡t kÃª táº¥t cáº£ [title | Äiá»u X, Khoáº£n Y]
- Má»—i cÄƒn cá»© kÃ¨m tÃ³m táº¯t ná»™i dung

# Cáº§n tham kháº£o thÃªm (náº¿u cÃ³)
- Nhá»¯ng quy Ä‘á»‹nh chÆ°a cÃ³ trong CONTEXT
- VÄƒn báº£n phÃ¡p luáº­t liÃªn quan khÃ¡c

# LÆ°u Ã½ quan trá»ng
- ThÃ´ng tin mang tÃ­nh tham kháº£o
- NÃªn tham kháº£o luáº­t sÆ° Ä‘á»ƒ tÆ° váº¥n cá»¥ thá»ƒ

CONTEXT:
{context}

CÃ‚U Há»I:
{question}
'''
    try:
        model = genai.GenerativeModel(model_id)
        res = model.generate_content(prompt)
        return (res.text or "").strip()
    except Exception:
        # Fallback: thá»­ model 1.5 náº¿u 2.0 khÃ´ng kháº£ dá»¥ng
        try:
            model = genai.GenerativeModel("gemini-1.5-flash")
            res = model.generate_content(prompt)
            return (res.text or "").strip()
        except Exception as e2:
            print("Gemini error:", e2)
            return None

# -----------------------------
# Routes
# -----------------------------
@app.route("/")
def index():
    return send_from_directory(str(APP_DIR), "index.html")

@app.route("/ask", methods=["POST"])
def ask():
    data = request.get_json() or {}
    question = (data.get("question") or "").strip()
    if not question:
        return jsonify({"status": "error", "error": "Vui lÃ²ng nháº­p cÃ¢u há»i"}), 400

    t_all0 = time.time()

    # ===== ONLINE BRANCH: Retrieval-only -> Gemini =====
    if _online() and _gemini_enabled():
        print("ğŸŒ Sá»­ dá»¥ng cháº¿ Ä‘á»™ ONLINE vá»›i Gemini")
        print_status_info(True, "gemini", os.getenv("GEMINI_MODEL", "gemini-2.0-flash"), _head(question), "")
        
        # 1) Retrieval-only Ä‘á»ƒ láº¥y citations
        print("ğŸ“š Äang thá»±c hiá»‡n retrieval...")
        t_ret0 = time.time()
        retrieval_only = Settings()
        retrieval_only.llm_enabled = False  # khÃ´ng gá»i LLM Ollama
        rag = answer_question(question, settings=retrieval_only)
        t_ret_ms = round((time.time() - t_ret0) * 1000, 2)
        print_step_timing("Retrieval hoÃ n thÃ nh", t_ret_ms)

        citations = rag.get("citations", [])
        context = _citations_to_context(citations)

        # 2) Gá»i Gemini
        print("ğŸš€ Äang gá»­i yÃªu cáº§u Ä‘áº¿n Gemini...")
        t_llm0 = time.time()
        ans = _gemini_answer(question, context)
        t_llm_ms = round((time.time() - t_llm0) * 1000, 2)
        print_step_timing("Gemini xá»­ lÃ½", t_llm_ms)

        if ans:
            total_sec = round((time.time() - t_all0), 2)
            timing_data = {
                "retrieval_ms": t_ret_ms,
                "llm_ms": t_llm_ms,
                "total_ms": round(total_sec * 1000, 2),
            }
            print_timing_info(timing_data)
            
            return jsonify({
                "status": "success",
                "mode": "gemini-online",
                "answer": ans,
                "citations": citations,
                "latency_sec": total_sec,
                # META
                "ai": "gemini",
                "model": os.getenv("GEMINI_MODEL", "gemini-2.0-flash"),
                "question_head": _head(question),
                "context_head": _head(context),
                "timings": timing_data,
            })

    # ===== OFFLINE BRANCH: Ollama pipeline (giá»¯ nguyÃªn logic) =====
    print("ğŸ’» Sá»­ dá»¥ng cháº¿ Ä‘á»™ OFFLINE vá»›i Ollama")
    t_pipe0 = time.time()
    rag = answer_question(question, settings=settings)
    t_total_ms = round((time.time() - t_pipe0) * 1000, 2)

    # cá»‘ gáº¯ng láº¥y context Ä‘áº§u náº¿u cÃ³
    citations = rag.get("citations", [])
    context = _citations_to_context(citations)
    
    # Sá»­ dá»¥ng timing data tá»« pipeline náº¿u cÃ³
    timing_data = rag.get("timings", {
        "retrieval_ms": None,
        "llm_ms": None,
        "total_ms": t_total_ms,
    })

    return jsonify({
        "status": "success",
        "mode": "ollama-offline",
        "answer": rag.get("answer",""),
        "citations": citations,
        "latency_sec": round((time.time() - t_all0), 2),
        # META
        "ai": "ollama",
        "model": os.getenv("LLM_MODEL", "qwen2.5:3b-instruct"),
        "question_head": _head(question),
        "context_head": _head(context),
        "timings": timing_data,
    })

@app.route("/health")
def health():
    return jsonify({
        "status": "healthy",
        "internet": _online(),
        "gemini_configured": _gemini_enabled(),
        "ollama_configured": True,
        "timestamp": time.time()
    })

if __name__ == "__main__":
    print("ğŸš€ Khá»Ÿi Ä‘á»™ng VN Legal Assistant")
    print("ğŸ“š Loading indexâ€¦")
    load_index(settings.data_dir)
    print("âœ… Ready at http://localhost:5000")
    app.run(host="0.0.0.0", port=5000, debug=True)