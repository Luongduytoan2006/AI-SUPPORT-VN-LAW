import os, time, requests
from pathlib import Path
from flask import Flask, send_from_directory, request, jsonify
import google.generativeai as genai

from core.pipeline import load_index, answer_question
from core.settings import Settings
from core.utils import print_status_info, print_step_timing, print_timing_info, check_internet_connection

ROOT = Path(__file__).resolve().parents[1]  # project root
APP_DIR = ROOT / "app"
GEMINI_PROMPT_PATH = ROOT / "prompts" / "gemini_answer.txt"

app = Flask(__name__, static_folder=str(APP_DIR), template_folder=str(APP_DIR))
app.secret_key = os.getenv('SECRET_KEY', 'vn-legal-assistant-2024')

settings = Settings()

# -----------------------------
# Helpers
# -----------------------------
def _online() -> bool:
    return check_internet_connection()

def _gemini_enabled() -> bool:
    return bool(os.getenv("GOOGLE_API_KEY", "").strip())

def _citations_to_context(citations) -> str:
    buf = []
    for c in citations or []:
        title  = c.get("title","")
        art    = c.get("article","")
        clause = c.get("clause")
        text   = (c.get("text") or "").strip()
        src    = c.get("source","")
        tag = f"[{title} | Äiá»u {art}" + (f", Khoáº£n {clause}]" if clause else "]")
        buf.append(f"{tag}\n{text}\nSOURCE: {src}")
    return "\n---\n".join(buf)

def _head(s: str, n: int = 50) -> str:
    s = (s or "").strip().replace("\n", " ")
    return s[:n]

def _gemini_answer(question: str, context: str) -> str | None:
    key = os.getenv("GOOGLE_API_KEY", "").strip()
    if not key:
        return None
    model_id = os.getenv("GEMINI_MODEL", "gemini-2.0-flash")
    genai.configure(api_key=key)
    
    # Äá»c system prompt tá»« file
    try:
        system_prompt = GEMINI_PROMPT_PATH.read_text(encoding="utf-8")
    except Exception:
        # Fallback prompt náº¿u file khÃ´ng tá»“n táº¡i
        system_prompt = (
            "Báº¡n lÃ  Luáº­t sÆ° tÆ° váº¥n phÃ¡p luáº­t Viá»‡t Nam chuyÃªn nghiá»‡p. "
            "TÆ° váº¥n dá»±a trÃªn CONTEXT chÃ­nh xÃ¡c vÃ  thá»±c tiá»…n. "
            "CHá»ˆ sá»­ dá»¥ng thÃ´ng tin cÃ³ trong CONTEXT. "
            "Má»—i káº¿t luáº­n pháº£i cÃ³ trÃ­ch dáº«n vÄƒn báº£n phÃ¡p luáº­t Ä‘áº§y Ä‘á»§ (Luáº­t/Nghá»‹ Ä‘á»‹nh | Äiá»u X, Khoáº£n Y). "
            "Tráº£ lá»i Ä‘áº§y Ä‘á»§, chi tiáº¿t nhÆ° luáº­t sÆ° chuyÃªn nghiá»‡p."
        )
    
    prompt = f'''{system_prompt}

CONTEXT:
{context}

CÃ‚U Há»I:
{question}
'''
    try:
        model = genai.GenerativeModel(model_id)
        res = model.generate_content(prompt)
        return (res.text or "").strip()
    except Exception:
        # Fallback: thá»­ model 1.5 náº¿u 2.0 khÃ´ng kháº£ dá»¥ng
        try:
            model = genai.GenerativeModel("gemini-1.5-flash")
            res = model.generate_content(prompt)
            return (res.text or "").strip()
        except Exception as e2:
            print("Gemini error:", e2)
            return None

# -----------------------------
# Routes
# -----------------------------
@app.route("/")
def index():
    return send_from_directory(str(APP_DIR), "index.html")

@app.route("/ask", methods=["POST"])
def ask():
    data = request.get_json() or {}
    question = (data.get("question") or "").strip()
    if not question:
        return jsonify({"status": "error", "error": "Vui lÃ²ng nháº­p cÃ¢u há»i"}), 400

    t_all0 = time.time()

    # ===== ONLINE BRANCH: Retrieval-only -> Gemini =====
    if _online() and _gemini_enabled():
        print("ğŸŒ Sá»­ dá»¥ng cháº¿ Ä‘á»™ ONLINE vá»›i Gemini")
        print_status_info(True, "gemini", os.getenv("GEMINI_MODEL", "gemini-2.0-flash"), _head(question), "")
        
        # 1) Retrieval-only Ä‘á»ƒ láº¥y citations
        print("ğŸ“š Äang thá»±c hiá»‡n retrieval...")
        t_ret0 = time.time()
        retrieval_only = Settings()
        retrieval_only.llm_enabled = False  # khÃ´ng gá»i LLM Ollama
        rag = answer_question(question, settings=retrieval_only)
        t_ret_ms = round((time.time() - t_ret0) * 1000, 2)
        print_step_timing("Retrieval hoÃ n thÃ nh", t_ret_ms)

        citations = rag.get("citations", [])
        context = _citations_to_context(citations)

        # 2) Gá»i Gemini
        print("ğŸš€ Äang gá»­i yÃªu cáº§u Ä‘áº¿n Gemini...")
        t_llm0 = time.time()
        ans = _gemini_answer(question, context)
        t_llm_ms = round((time.time() - t_llm0) * 1000, 2)
        print_step_timing("Gemini xá»­ lÃ½", t_llm_ms)

        if ans:
            total_sec = round((time.time() - t_all0), 2)
            timing_data = {
                "retrieval_ms": t_ret_ms,
                "llm_ms": t_llm_ms,
                "total_ms": round(total_sec * 1000, 2),
            }
            print_timing_info(timing_data)
            
            return jsonify({
                "status": "success",
                "mode": "gemini-online",
                "answer": ans,
                "citations": citations,
                "latency_sec": total_sec,
                # META
                "ai": "gemini",
                "model": os.getenv("GEMINI_MODEL", "gemini-2.0-flash"),
                "question_head": _head(question),
                "context_head": _head(context),
                "timings": timing_data,
            })

    # ===== OFFLINE BRANCH: Ollama pipeline (giá»¯ nguyÃªn logic) =====
    print("ğŸ’» Sá»­ dá»¥ng cháº¿ Ä‘á»™ OFFLINE vá»›i Ollama")
    t_pipe0 = time.time()
    rag = answer_question(question, settings=settings)
    t_total_ms = round((time.time() - t_pipe0) * 1000, 2)

    # cá»‘ gáº¯ng láº¥y context Ä‘áº§u náº¿u cÃ³
    citations = rag.get("citations", [])
    context = _citations_to_context(citations)
    
    # Sá»­ dá»¥ng timing data tá»« pipeline náº¿u cÃ³
    timing_data = rag.get("timings", {
        "retrieval_ms": None,
        "llm_ms": None,
        "total_ms": t_total_ms,
    })

    return jsonify({
        "status": "success",
        "mode": "ollama-offline",
        "answer": rag.get("answer",""),
        "citations": citations,
        "latency_sec": round((time.time() - t_all0), 2),
        # META
        "ai": "ollama",
        "model": os.getenv("LLM_MODEL", "qwen2.5:3b-instruct"),
        "question_head": _head(question),
        "context_head": _head(context),
        "timings": timing_data,
    })

@app.route("/health")
def health():
    return jsonify({
        "status": "healthy",
        "internet": _online(),
        "gemini_configured": _gemini_enabled(),
        "ollama_configured": True,
        "timestamp": time.time()
    })

if __name__ == "__main__":
    print("ğŸš€ Khá»Ÿi Ä‘á»™ng AURA Legal")
    print("ğŸ“š Loading indexâ€¦")
    load_index(settings.data_dir)
    print("âœ… Ready at http://localhost:5000")
    app.run(host="0.0.0.0", port=5000, debug=True)