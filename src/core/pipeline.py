from __future__ import annotations
from typing import List, Dict
import os, re, time, pathlib

# BM25
from core.retrieval.bm25_json import (
    load_index as bm25_load_index,
    reload_index as bm25_reload_index,
    search as bm25_search,
    available_units,
)
# Vector JSONL
from core.retrieval.vector_jsonl import load_vector_store, vector_search

from core.settings import Settings
from core.llm_client import chat
from core.utils import Heartbeat, lap_timer, print_step_timing, print_timing_info, check_internet_connection, print_status_info

ROOT = pathlib.Path(__file__).resolve().parents[1]
PROMPT_PATH = ROOT / "prompts" / "final_answer.txt"

# ---- Heuristics rÃºt gá»n Ä‘á»ƒ thu háº¹p pháº¡m vi theo tá»« khÃ³a ----
# NOTE: ÄÃ¢y lÃ  vÃ­ dá»¥ cho dá»¯ liá»‡u máº«u phÃ¡p luáº­t Viá»‡t Nam
# Náº¿u báº¡n dÃ¹ng dá»¯ liá»‡u riÃªng, hÃ£y tÃ¹y chá»‰nh hoáº·c comment láº¡i pháº§n nÃ y
_KEY2TITLE = [
    (re.compile(r"\b(shtt|sá»Ÿ\s*há»¯u\s*trÃ­\s*tuá»‡|nhÃ£n\s*hiá»‡u|báº£n\s*quyá»n|sÃ¡ng\s*cháº¿|bÃ­\s*máº­t\s*kinh\s*doanh)\b", re.I), "so_huu_tri_tue"),
    (re.compile(r"\b(Ä‘áº¥t\s*Ä‘ai|sá»•\s*Ä‘á»|giáº¥y\s*chá»©ng\s*nháº­n)\b", re.I), "dat_dai"),
    (re.compile(r"\b(lao\s*Ä‘á»™ng|há»£p\s*Ä‘á»“ng\s*lao\s*Ä‘á»™ng|sa\s*tháº£i)\b", re.I), "lao_dong"),
    (re.compile(r"\b(hÃ´n\s*nhÃ¢n|ly\s*hÃ´n|con\s*chung)\b", re.I), "hon_nhan"),
    (re.compile(r"\b(giao\s*thÃ´ng|ná»“ng\s*Ä‘á»™\s*cá»“n|xá»­\s*pháº¡t|má»©c\s*pháº¡t|pháº¡t)\b", re.I), "giao_thong_duong_bo"),
    (re.compile(r"\b(an\s*ninh\s*máº¡ng|khÃ´ng\s*gian\s*máº¡ng)\b", re.I), "an_ninh_mang"),
]
# Äá»ƒ táº¯t tÃ­nh nÄƒng nÃ y (tÃ¬m kiáº¿m toÃ n bá»™ dá»¯ liá»‡u), Ä‘áº·t: _KEY2TITLE = []

def _restrict_titles_for_penalty(all_titles: List[str]) -> List[str]:
    pref = [t for t in all_titles if t.startswith("xu_phat_")]
    return pref[:2] if pref else []

def _pick_titles(question: str, data_dir: str) -> List[str]:
    titles = {os.path.splitext(n)[0] for n in os.listdir(data_dir) if n.lower().endswith(".json")}
    if re.search(r"\b(má»©c\s*pháº¡t|xá»­\s*pháº¡t|pháº¡t|tiá»n\s*pháº¡t)\b", (question or ""), re.I):
        pref = _restrict_titles_for_penalty(sorted(titles))
        if pref:
            return pref
    chosen: List[str] = [name for (rg, name) in _KEY2TITLE if rg.search(question or "") and name in titles]
    return list(dict.fromkeys(chosen))[:2]

def _format_context(hits: List[Dict], max_chars: int) -> str:
    buf, total = [], 0
    for h in hits:
        head = f"[{h['title']} | Äiá»u {h['article']}" + (f", Khoáº£n {h.get('clause')}]" if h.get('clause') else "]")
        body = f"{head}\n{h.get('text','')}\nSOURCE: {h.get('source')}\n"
        if total + len(body) > max_chars:
            break
        buf.append(body)
        total += len(body)
    return "\n---\n".join(buf)

def _direct_cite(hits: List[Dict]) -> Dict:
    bullets = []
    for h in hits:
        tag = f"[{h['title']} | Äiá»u {h['article']}" + (f", Khoáº£n {h.get('clause')}]" if h.get('clause') else "]")
        score = h.get("score") if h.get("score") is not None else h.get("vscore")
        bullets.append(f"- **{tag}** â€” {h.get('text','')[:220].strip()} â€¦ (score: {round(float(score or 0.0),4)})")
    answer = (
        "# Káº¿t quáº£ trÃ­ch dáº«n nhanh\n" + "\n".join(bullets) +
        "\n\n> *LÆ°u Ã½:* ÄÃ¢y lÃ  trÃ­ch dáº«n tá»± Ä‘á»™ng; Ä‘á»ƒ phÃ¢n tÃ­ch tÃ¬nh huá»‘ng, báº­t cháº¿ Ä‘á»™ phÃ¢n tÃ­ch."
    )
    return {"mode": "direct-cite", "answer": answer, "citations": hits, "used_context": True}

def _rag_answer(question: str, ctx: str, settings: Settings) -> str:
    try:
        sys_prompt = PROMPT_PATH.read_text(encoding="utf-8")
    except Exception:
        sys_prompt = (
            "Báº¡n lÃ  Luáº­t sÆ° tÆ° váº¥n phÃ¡p luáº­t Viá»‡t Nam chuyÃªn nghiá»‡p. "
            "Sá»­ dá»¥ng CONTEXT Ä‘á»ƒ tÆ° váº¥n chÃ­nh xÃ¡c vÃ  thá»±c tiá»…n. "
            "Má»—i káº¿t luáº­n pháº£i cÃ³ trÃ­ch dáº«n [title | Äiá»u X, Khoáº£n Y]. "
            "Tráº£ lá»i Ä‘áº§y Ä‘á»§, chi tiáº¿t nhÆ° má»™t luáº­t sÆ° chuyÃªn nghiá»‡p. "
            "Náº¿u thiáº¿u thÃ´ng tin, ghi 'Cáº§n tham kháº£o thÃªm' vÃ  nÃªu rÃµ cáº§n gÃ¬. "
            "DÃ¹ng markdown cÃ³ cáº¥u trÃºc rÃµ rÃ ng."
        )
    messages = [
        {"role": "system", "content": sys_prompt},
        {"role": "user", "content": f"CONTEXT:\n{ctx}\n\nCÃ‚U Há»I: {question}"},
    ]
    model       = getattr(settings, "llm_model",  os.getenv("LLM_MODEL", "qwen2.5:3b-instruct"))
    max_tokens  = getattr(settings, "max_tokens", int(os.getenv("MAX_TOKENS", "400")))
    temperature = getattr(settings, "temperature", 0.0)

    hb_sec = float(os.getenv("HEARTBEAT_SEC", "60"))
    with Heartbeat("Äang gá»i LLM", every=hb_sec):
        return chat(messages, model=model, max_tokens=max_tokens, temperature=temperature)

def _rrf_merge(bm25_hits: List[Dict], vec_hits: List[Dict], top_k: int = 6, k: float = 60.0) -> List[Dict]:
    pool: Dict[tuple, Dict] = {}
    for rank, h in enumerate(bm25_hits, 1):
        key = (h["title"], h["article"], h.get("clause"), h["source"])
        pool.setdefault(key, {"item": h.copy(), "rrf": 0.0, "bm_r": rank, "ve_r": 10**9})
        pool[key]["rrf"] += 1.0 / (k + rank)
    for rank, h in enumerate(vec_hits, 1):
        key = (h["title"], h["article"], h.get("clause"), h["source"])
        pool.setdefault(key, {"item": h.copy(), "rrf": 0.0, "bm_r": 10**9, "ve_r": rank})
        pool[key]["rrf"] += 1.0 / (k + rank)
        if "score" not in pool[key]["item"] and h.get("vscore") is not None:
            pool[key]["item"]["score"] = h["vscore"]
    merged = sorted(pool.values(), key=lambda x: (-x["rrf"], x["bm_r"], x["ve_r"]))
    out = []
    for x in merged[:top_k]:
        it = x["item"]
        it["rrf"] = round(x["rrf"], 6)
        out.append(it)
    return out

def load_index(data_dir: str) -> int:
    n = bm25_load_index(data_dir)
    if os.getenv("EMBEDDINGS_ENABLED", "true").lower() == "true":
        try:
            index_path = os.getenv("INDEX_PATH", "index/index.jsonl")
            m = load_vector_store(index_path)
            print(f"[vector] loaded {m} units from {index_path}")
        except Exception as e:
            print(f"[vector] skip: {e}")
    return n

def reload_index(data_dir: str) -> int:
    n = bm25_reload_index(data_dir)
    if os.getenv("EMBEDDINGS_ENABLED", "true").lower() == "true":
        try:
            index_path = os.getenv("INDEX_PATH", "index/index.jsonl")
            m = load_vector_store(index_path)
            print(f"[vector] reloaded {m} units from {index_path}")
        except Exception as e:
            print(f"[vector] skip reload: {e}")
    return n

def answer_question(question: str, settings: Settings) -> Dict:
    t_all = time.perf_counter()
    
    # Kiá»ƒm tra káº¿t ná»‘i internet
    is_online = check_internet_connection()
    ai_type = "gemini" if is_online and os.getenv("GOOGLE_API_KEY", "").strip() else "ollama"
    model_name = os.getenv("GEMINI_MODEL", "gemini-2.0-flash") if ai_type == "gemini" else os.getenv("LLM_MODEL", "qwen2.5:3b-instruct")
    
    # In thÃ´ng tin tráº¡ng thÃ¡i
    print_status_info(is_online, ai_type, model_name, question[:50], "")
    
    # Chá»n titles
    print("ğŸ” Äang chá»n pháº¡m vi luáº­t phÃ¹ há»£p...")
    t_title0 = time.perf_counter()
    chosen_titles = _pick_titles(question, settings.data_dir)
    t_title = (time.perf_counter() - t_title0) * 1000.0
    print_step_timing("Chá»n pháº¡m vi luáº­t", t_title)
    
    # BM25 search
    print("ğŸ“ Äang tÃ¬m kiáº¿m tá»« khÃ³a (BM25)...")
    t_bm0 = time.perf_counter()
    bm = bm25_search(question, top_k=settings.top_k, allow_titles=chosen_titles or None)
    t_bm = (time.perf_counter() - t_bm0) * 1000.0
    print_step_timing("TÃ¬m kiáº¿m BM25", t_bm)

    # Vector search
    vc: List[Dict] = []
    t_vec = 0.0
    if os.getenv("EMBEDDINGS_ENABLED", "true").lower() == "true":
        print("ğŸ¯ Äang tÃ¬m kiáº¿m ngá»¯ nghÄ©a (Vector)...")
        t_v0 = time.perf_counter()
        try:
            vc = vector_search(
                question,
                top_k=settings.top_k,
                allow_titles=chosen_titles or None,
                embed_model=os.getenv("EMBED_MODEL", "nomic-embed-text"),
            )
        finally:
            t_vec = (time.perf_counter() - t_v0) * 1000.0
            print_step_timing("TÃ¬m kiáº¿m Vector", t_vec)

    # Merge results  
    print("ğŸ”„ Äang há»£p nháº¥t káº¿t quáº£ tÃ¬m kiáº¿m...")
    t_merge0 = time.perf_counter()
    hits = _rrf_merge(bm, vc, top_k=settings.top_k)
    t_merge = (time.perf_counter() - t_merge0) * 1000.0
    print_step_timing("Há»£p nháº¥t káº¿t quáº£", t_merge)
    # Format context
    print("ğŸ“„ Äang Ä‘á»‹nh dáº¡ng ngá»¯ cáº£nh...")
    t_ctx0 = time.perf_counter()
    ctx = _format_context(hits, settings.max_context_chars)
    t_ctx = (time.perf_counter() - t_ctx0) * 1000.0
    print_step_timing("Äá»‹nh dáº¡ng ngá»¯ cáº£nh", t_ctx)
    
    # In thÃ´ng tin context sau khi cÃ³
    if ctx:
        print_status_info(is_online, ai_type, model_name, question[:50], ctx[:50])

    # router direct-cite?
    direct_kw = re.compile(r"\b(Ä‘iá»u\s+\d+|khoáº£n\s+\d+|trÃ­ch|khÃ¡i\s*niá»‡m|Ä‘á»‹nh\s*nghÄ©a|má»©c\s*pháº¡t|xá»­\s*pháº¡t|pháº¡t)\b", re.I)
    use_direct = settings.direct_cite_first and (direct_kw.search(question) or not settings.llm_enabled)

    if use_direct or not ctx.strip():
        total = (time.perf_counter() - t_all) * 1000.0
        print("âœ… Sá»­ dá»¥ng cháº¿ Ä‘á»™ trÃ­ch dáº«n trá»±c tiáº¿p")
        out = _direct_cite(hits)
        
        timing_data = {
            "bm25_ms": round(t_bm, 2),
            "vector_ms": round(t_vec, 2),
            "llm_ms": 0.0,
            "total_ms": round(total, 2),
        }
        print_timing_info(timing_data)
        
        out.update({
            "chosen_titles": chosen_titles,
            "available_units": available_units(),
            "latency_ms": int(total),
            "timings": timing_data,
            "question_head": (question or "")[:50],
            "context_head": (ctx or "")[:50],
            "model": model_name,
            "ai": ai_type,
        })
        return out

    # LLM Processing
    print(f"ğŸ¤– Äang gá»­i yÃªu cáº§u Ä‘áº¿n AI ({ai_type.upper()})...")
    t_llm0 = time.perf_counter()
    content = _rag_answer(question, ctx, settings)
    t_llm = (time.perf_counter() - t_llm0) * 1000.0
    print_step_timing(f"Xá»­ lÃ½ AI ({ai_type.upper()})", t_llm)
    
    total = (time.perf_counter() - t_all) * 1000.0
    
    timing_data = {
        "bm25_ms": round(t_bm, 2),
        "vector_ms": round(t_vec, 2),
        "llm_ms": round(t_llm, 2),
        "total_ms": round(total, 2),
    }
    print_timing_info(timing_data)

    return {
        "mode": "rag+llm",
        "answer": content,
        "citations": hits,
        "used_context": True,
        "chosen_titles": chosen_titles,
        "available_units": available_units(),
        "latency_ms": int(total),
        "timings": timing_data,
        "question_head": (question or "")[:50],
        "context_head": (ctx or "")[:50],
        "model": model_name,
        "ai": ai_type,
    }
